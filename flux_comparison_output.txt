D:\comfy2\Lib\site-packages\torch\cuda\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
====================================================================================================
FLUX FP8 MODEL ANALYSIS
====================================================================================================

1. METADATA
====================================================================================================
  _quantization_metadata: <6577 bytes>
    Format version: 1.0
    Layers count: 112
    First layer example: double_blocks.0.img_attn.proj
      Config: {'format': 'float8_e4m3fn'}

2. TENSOR COUNTS
====================================================================================================
Total tensors: 425
  .weight tensors: 121
  .scale_weight tensors: 0
  .weight_scale tensors: 112
  .weight_scale_2 tensors: 0
  .input_scale tensors: 112
  .bias tensors: 0
  norm tensors: 80
  other tensors: 0

3. MARKER TENSORS
====================================================================================================
No marker tensors found (no short names without dots)

4. SAMPLE QUANTIZED LAYER
====================================================================================================
Layer: double_blocks.0.img_attn.proj.weight
  .weight shape: [4096, 4096], dtype: torch.float8_e4m3fn
  .weight_scale: shape=[], dtype=torch.float32
  .input_scale: shape=[], dtype=torch.float32, value=0.0803571417927742
  Weight is FP8 E4M3FN
  Value range: [-416.000000, 448.000000]

====================================================================================================
FLUX NVFP4 MODEL ANALYSIS
====================================================================================================

1. METADATA
====================================================================================================
  _quantization_metadata: <5575 bytes>
    Format version: 1.0
    Layers count: 110
    First layer example: double_blocks.0.img_attn.proj
      Config: {'format': 'nvfp4'}

2. TENSOR COUNTS
====================================================================================================
Total tensors: 531
  .weight tensors: 121
  .scale_weight tensors: 0
  .weight_scale tensors: 110
  .weight_scale_2 tensors: 110
  .input_scale tensors: 110
  .bias tensors: 0
  norm tensors: 80
  other tensors: 0

3. MARKER TENSORS
====================================================================================================
No marker tensors found (no short names without dots)

4. SAMPLE QUANTIZED LAYER
====================================================================================================
Layer: double_blocks.0.img_attn.proj.weight
  .weight shape: [4096, 2048], dtype: torch.uint8
  .weight_scale: shape=[4096, 256], dtype=torch.float8_e4m3fn
    First 5 values: [48.0, 72.0, 72.0, 72.0, 56.0]
    Range: [12.000000, 448.000000]
  .weight_scale_2: shape=[], dtype=torch.float32, value=0.0001354217529296875
  .input_scale: shape=[], dtype=torch.float32, value=0.013392857275903225
  Weight is UINT8 (packed NVFP4)
  Packed weight is half size of expected (2 FP4 per byte)

====================================================================================================
COMPARISON
====================================================================================================

1. TENSOR NAMING DIFFERENCES
FP8 quantized layers: 121
NVFP4 quantized layers: 121

FP8 sample layer: double_stream_modulation_img.lin
Traceback (most recent call last):
  File "D:\comfy2\ComfyUI\nvfp4-conv\compare_flux_models.py", line 311, in <module>
    print(f"  {suffix}: {'\u2713' if exists else '\u2717'}")
  File "C:\Users\joris\AppData\Local\Programs\Python\Python312\Lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'charmap' codec can't encode character '\u2713' in position 11: character maps to <undefined>
